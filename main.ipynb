{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, input_size, attention_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.query_layer = torch.nn.Linear(input_size, attention_size)\n",
    "        self.key_layer = torch.nn.Linear(input_size, attention_size)\n",
    "        self.value_layer = torch.nn.Linear(input_size, attention_size)\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(attention_size, input_size)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \n",
    "        query = self.query_layer(query)\n",
    "        key = self.key_layer(key)\n",
    "        value = self.value_layer(value)\n",
    "\n",
    "        attention = torch.bmm(query, key.transpose(1, 2))\n",
    "        attention = attention / (attention.size(-1) ** 0.5)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        # print(attention)\n",
    "\n",
    "        '''\n",
    "        The mask is a binary tensor with the same shape as the input sequence, \n",
    "        where the elements that should be attended to are set to 1, and the elements \n",
    "        that should be ignored are set to 0. If mask is provided, the attention_scores \n",
    "        tensor is updated with the mask by replacing the attention scores corresponding \n",
    "        to the position of 0 with a large negative value (-1e9) before applying the \n",
    "        softmax function. This ensures that the attention weights will be close to zero \n",
    "        for the elements that are masked.\n",
    "        '''\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # print(attention)\n",
    "\n",
    "        # print(f'X size {attention.size()}')\n",
    "        # print(f'V size {value.size()}')\n",
    "        output = torch.bmm(attention, value)\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# a = SelfAttention(4, 10)\n",
    "\n",
    "# # batch size, sequnce length, input size\n",
    "# x = torch.randn(1, 3, 4)\n",
    "# m = torch.tensor([[1,0,0], [0,1,0], [0,0,1]])\n",
    "# y = a(x, mask=m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what transformer look like\n",
    "<img src=\"transformer_architecture.png\" style=\"width:30%; height:30%; display:block;\"/>\n",
    "and here's how we're implementing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))  \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention1 = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.attention2 = SelfAttention(embed_size, heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention1(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm1(attention + x))\n",
    "\n",
    "        attention = self.attention2(value, key, query, src_mask)\n",
    "        x = self.dropout(self.norm2(attention + query))\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm3(forward + x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        max_length=100,\n",
    "        embed_size=256,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_word_embedding = TokenEmbedding(src_vocab_size, embed_size)\n",
    "        self.encoder_position_embedding = PositionalEncoding(embed_size, 0, max_length)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    embed_size, \n",
    "                    heads, \n",
    "                    forward_expansion,\n",
    "                    dropout, \n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_word_embedding = TokenEmbedding(trg_vocab_size, embed_size)\n",
    "        self.decoder_position_embedding = PositionalEncoding(embed_size, 0, max_length)\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
    "        src = self.encoder_word_embedding(src)\n",
    "        src = self.encoder_position_embedding(src)\n",
    "\n",
    "        # src_number, src_length = src.shape\n",
    "        # src_positions = torch.arange(0, src_length).expand(src_number, src_length)\n",
    "        # src = self.dropout(self.encoder_word_embedding(src) + self.encoder_position_embedding(src_positions))\n",
    "\n",
    "        for encoder in self.encoder_blocks:\n",
    "            src = encoder(src, src, src, src_mask)\n",
    "\n",
    "        trg = self.decoder_word_embedding(trg)\n",
    "        trg = self.decoder_position_embedding(trg)\n",
    "\n",
    "        # trg_number, trg_length = trg.shape\n",
    "        # trg_positions = torch.arange(0, trg_length).expand(trg_number, trg_length)\n",
    "        # trg = self.dropout(self.decoder_word_embedding(trg) + self.decoder_position_embedding(trg_positions))\n",
    "\n",
    "        for decoder in self.decoder_blocks:\n",
    "            trg = decoder(trg, src, src, src_mask, trg_mask)\n",
    "\n",
    "        out = torch.softmax(self.fc_out(trg), dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Chinese text file\n",
    "with open(\"../data/translation/chinese.txt\", encoding=\"utf8\") as f:\n",
    "    chinese_text = f.readlines()\n",
    "\n",
    "# Read the English text file\n",
    "with open(\"../data/translation/english.txt\", encoding=\"utf8\") as f:\n",
    "    english_text = f.readlines()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = chinese_text[1]\n",
    "en = english_text[1]\n",
    "\n",
    "print(ch)\n",
    "\n",
    "ch_token = torch.tensor([tokenizer(ch)['input_ids']])\n",
    "en_token = tokenizer(en)['input_ids']\n",
    "\n",
    "print(ch_token.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45122305\n"
     ]
    }
   ],
   "source": [
    "t = Transformer(50257, 50257)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 21])\n",
      "tensor([[[0.0964, 0.0528, 0.0289, 0.0235, 0.0483, 0.0395, 0.0616, 0.0681,\n",
      "          0.0569, 0.0507, 0.0420, 0.0380, 0.0332, 0.0576, 0.0226, 0.0661,\n",
      "          0.0255, 0.0332, 0.0598, 0.0423, 0.0531],\n",
      "         [0.1113, 0.1418, 0.0331, 0.0305, 0.0247, 0.0150, 0.0160, 0.0624,\n",
      "          0.0692, 0.0240, 0.0359, 0.0528, 0.0277, 0.0292, 0.0461, 0.0395,\n",
      "          0.0213, 0.0478, 0.0512, 0.0440, 0.0766],\n",
      "         [0.0496, 0.0725, 0.0397, 0.0264, 0.0440, 0.0345, 0.0361, 0.0908,\n",
      "          0.0857, 0.0493, 0.0414, 0.0308, 0.0241, 0.0412, 0.0202, 0.0395,\n",
      "          0.0377, 0.0364, 0.0408, 0.0435, 0.1160],\n",
      "         [0.0821, 0.0637, 0.0195, 0.0186, 0.0397, 0.0431, 0.0262, 0.0649,\n",
      "          0.1942, 0.0473, 0.0451, 0.0548, 0.0192, 0.0354, 0.0190, 0.0218,\n",
      "          0.0156, 0.0476, 0.0551, 0.0331, 0.0538],\n",
      "         [0.0894, 0.0640, 0.0305, 0.0216, 0.0294, 0.0338, 0.0463, 0.1095,\n",
      "          0.0495, 0.0478, 0.0291, 0.0217, 0.0581, 0.0610, 0.0101, 0.0291,\n",
      "          0.0333, 0.0903, 0.0333, 0.0431, 0.0691],\n",
      "         [0.1195, 0.0652, 0.0265, 0.0483, 0.0344, 0.0508, 0.0260, 0.0434,\n",
      "          0.0863, 0.0443, 0.0448, 0.0494, 0.0423, 0.0411, 0.0107, 0.0308,\n",
      "          0.0259, 0.0392, 0.0937, 0.0461, 0.0314],\n",
      "         [0.1207, 0.0653, 0.0268, 0.0478, 0.0345, 0.0522, 0.0263, 0.0446,\n",
      "          0.0872, 0.0445, 0.0437, 0.0478, 0.0426, 0.0411, 0.0107, 0.0298,\n",
      "          0.0264, 0.0384, 0.0922, 0.0456, 0.0317],\n",
      "         [0.1207, 0.0653, 0.0268, 0.0478, 0.0345, 0.0522, 0.0263, 0.0446,\n",
      "          0.0872, 0.0445, 0.0437, 0.0478, 0.0426, 0.0411, 0.0107, 0.0298,\n",
      "          0.0264, 0.0384, 0.0922, 0.0456, 0.0317]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.0528, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# src vocab size = 13\n",
    "# trg vocab size = 21\n",
    "t = Transformer(13,21)\n",
    "\n",
    "src = torch.tensor([[2,5,6,7,8,3,1,1]])\n",
    "trg = torch.tensor([[2,6,7,8,3,1,1,1]])\n",
    "output = t(src, trg)\n",
    "\n",
    "\n",
    "print(output.size())\n",
    "print(output)\n",
    "\n",
    "'''\n",
    "\n",
    "output[i][j][k] represents the probability of the k-th \n",
    "token in the target vocabulary being the next token after \n",
    "the j-th token in the input sequence, in the i-th example in the batch\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0331, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output[0][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929年还是1989年?\\n', '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\\n', '一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。\\n', '如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。\\n', '目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。\\n', '欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时机来实行急需的结构性改革。\\n', '然而，作为地域战略学家，无论是从政治意义还是从经济意义上，让我自然想到的年份是1989年。\\n', '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。\\n', '事实上，从表面上看，两者似乎是完全是相反的：一个是象征着压抑和人为分裂的柏林墙的倒塌，而另一个是看似坚不可摧的并令人安心的金融资本主义机构的倒塌。\\n', '然而，和1989年一样，2008-2009年很可能也能被视为一个划时代的改变，其带来的发人深省的后果将在几十年后仍能让我们感受得到。\\n']\n",
      "['1929 or 1989?\\n', 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\\n', 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.\\n', 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.\\n', 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).\\n', 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.\\n', 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.\\n', 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.\\n', 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.\\n', 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.\\n']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Read the Chinese text file\n",
    "with open(\"../data/translation/chinese.txt\", encoding=\"utf8\") as f:\n",
    "    chinese_text = f.readlines()\n",
    "\n",
    "# Read the English text file\n",
    "with open(\"../data/translation/english.txt\", encoding=\"utf8\") as f:\n",
    "    english_text = f.readlines()\n",
    "\n",
    "print(chinese_text[:10])\n",
    "print(english_text[:10])\n",
    "print('done')\n",
    "\n",
    "\n",
    "# # Convert the text to PyTorch tensors\n",
    "# english_tensor = torch.tensor([line for line in english_text])\n",
    "# chinese_tensor = torch.tensor([line for line in chinese_text])\n",
    "\n",
    "# # Print the first line of the text\n",
    "# print(english_text[0])\n",
    "# print(chinese_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-j-6B', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n",
      "巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
      "\n",
      "PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
      "\n",
      "[32432, 112, 165, 119, 236, 12, 49694, 237, 163, 251, 222, 163, 119, 237, 38184, 236, 39355, 109, 17312, 118, 38834, 23877, 255, 27950, 254, 162, 115, 109, 161, 240, 234, 164, 242, 241, 161, 119, 114, 171, 120, 234, 46763, 112, 10310, 103, 10310, 244, 45911, 234, 31660, 33566, 112, 28839, 101, 43380, 119, 33699, 122, 43889, 228, 20998, 110, 41468, 21410, 163, 109, 119, 27670, 120, 12859, 233, 20015, 114, 30585, 234, 17312, 249, 17312, 231, 27950, 102, 12859, 236, 22755, 239, 20015, 105, 12859, 228, 164, 100, 96, 33566, 106, 30298, 235, 29826, 96, 28839, 101, 20998, 239, 37955, 21410, 46349, 227, 37863, 113, 16764, 198]\n",
      "[27082, 1797, 784, 1081, 262, 3034, 4902, 2769, 641, 290, 9214, 641, 11, 262, 995, 468, 587, 10342, 329, 6754, 15075, 444, 284, 1037, 514, 1833, 644, 468, 587, 5836, 13, 198]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPTJModel\n",
    "\n",
    "\n",
    "\n",
    "# # model = torch.load('../models/GPT-J/GPT-J.bin')\n",
    "# # print(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "ch = chinese_text[1]\n",
    "en = english_text[1]\n",
    "\n",
    "print(ch)\n",
    "print(en)\n",
    "\n",
    "ch_token = tokenizer(ch)['input_ids']\n",
    "en_token = tokenizer(en)['input_ids']\n",
    "\n",
    "print(ch_token)\n",
    "print(en_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黎']\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor([[165, 119, 236]])\n",
    "word = tokenizer.batch_decode(tokens)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-j-6B', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "# x = 'My name is Eddy, and he is Matt'\n",
    "# a = tokenizer(x)\n",
    "# b = tokenizer(x, return_tensors='pt').input_ids\n",
    "\n",
    "# print(a)\n",
    "# print(b)\n",
    "\n",
    "# tokens = torch.tensor([[3666, 11, 1438, 11, 318, 11, 1717, 11, 9892, 11, 290, 11, 339, 11, 318, 11, 4705]])\n",
    "# word = tokenizer.batch_decode(tokens)\n",
    "# print(word)\n",
    "\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "# gen_tokens = model.generate(b, do_sample=True, temperature=0.9, max_length=100,)\n",
    "\n",
    "# print(gen_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset('wikitext', 'wikitext-103-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab()\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "\n",
    "# for i in range(70, 90):\n",
    "#     print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102068\n",
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=2, out_features=2, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=2, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=200, out_features=2, bias=True)\n",
      "        (norm1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=2, out_features=2, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=2, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=200, out_features=2, bias=True)\n",
      "        (norm1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(20000, 2)\n",
      "  (decoder): Linear(in_features=2, out_features=20000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ntokens = 20000  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "\n",
    "model = TransformerModel(ntokens, 2, nhead, d_hid, nlayers, dropout)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(count_parameters(model))\n",
    "print(model)\n",
    "\n",
    "# from torchtext.datasets import WikiText2\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# train_iter = WikiText2(split='train')\n",
    "# print(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_linear = nn.Linear(input_dim, attention_dim, bias=False)\n",
    "        self.key_linear = nn.Linear(input_dim, attention_dim, bias=False)\n",
    "        self.value_linear = nn.Linear(input_dim, attention_dim, bias=False)\n",
    "\n",
    "        self.output_linear = nn.Linear(attention_dim, input_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "\n",
    "        attention_weights = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.attention_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_weights, dim=2)\n",
    "        attention_output = torch.bmm(attention_weights, value)\n",
    "\n",
    "        y = self.output_linear(attention_output)\n",
    "\n",
    "        return y, attention_weights\n",
    "\n",
    "# a = SelfAttention(100, 100)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(count_parameters(a))\n",
    "# print(a)\n",
    "\n",
    "\n",
    "q_w = torch.tensor([[1,0,1], [1,0,0], [0,0,1], [0,1,1]])\n",
    "\n",
    "a = SelfAttention(4, 3)\n",
    "# a.query_linear.weight = torch.nn.Parameter()\n",
    "print(a.query_linear.weight.size())\n",
    "\n",
    "\n",
    "print(count_parameters(a))\n",
    "\n",
    "\n",
    "\n",
    "# print(count_parameters(t))\n",
    "# print(t)\n",
    "\n",
    "\n",
    "# e = nn.Embedding(3, 3)\n",
    "# x = torch.tensor([[1,0,2]])\n",
    "# print(e(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f331462f1a3623d9ad8b70946973c1dbbaf4cf768e0f82a62f2d70059232a38a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
